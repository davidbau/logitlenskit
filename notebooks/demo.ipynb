{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogitLensKit Demo\n",
    "\n",
    "This notebook demonstrates how to use LogitLensKit to visualize the logit lens of transformer language models.\n",
    "\n",
    "## What is the Logit Lens?\n",
    "\n",
    "The **Logit Lens** is an interpretability technique that decodes hidden states at each layer into vocabulary probabilities. By applying the model's output projection to intermediate layers, we can see how the model's predictions evolve through its computation.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, ensure you have the required dependencies:\n",
    "\n",
    "```bash\n",
    "cd python\n",
    "pip install -e \".[dev]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logitlenskit to path (for local development)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'python' / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from logitlenskit import collect_logit_lens_topk_efficient, show_logit_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "We'll start with GPT-2, which is small enough to run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('openai-community/gpt2', device_map='auto')\n",
    "print(f'Loaded {model.config.model_type} with {model.config.n_layer} layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Logit Lens Data\n",
    "\n",
    "The `collect_logit_lens_topk_efficient` function extracts:\n",
    "- Top-k predictions at each layer and token position\n",
    "- Probability trajectories showing how predictions evolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The capital of France is'\n",
    "\n",
    "data = collect_logit_lens_topk_efficient(\n",
    "    prompt,\n",
    "    model,\n",
    "    top_k=5,\n",
    "    track_across_layers=True,\n",
    "    remote=False,  # Set to True for NDIF remote execution\n",
    ")\n",
    "\n",
    "print(f'Collected data for {len(data[\"tokens\"])} tokens across {len(data[\"layers\"])} layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with Interactive Widget\n",
    "\n",
    "The `show_logit_lens` function creates an interactive visualization:\n",
    "\n",
    "- **Click cells** to see top-k predictions\n",
    "- **Click tokens** in the popup to pin their trajectories\n",
    "- **Click input tokens** (left column) to compare multiple positions\n",
    "- **Drag edges** to resize columns and chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(data, model.tokenizer, title=f'GPT-2: {prompt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Prompts\n",
    "\n",
    "Experiment with different prompts to see how the model's predictions evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'The quick brown fox jumps over the',\n",
    "    'To be or not to be, that is the',\n",
    "    '1 + 1 =',\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    data = collect_logit_lens_topk_efficient(\n",
    "        prompt, model, top_k=5, track_across_layers=True, remote=False\n",
    "    )\n",
    "    display(show_logit_lens(data, model.tokenizer, title=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NDIF for Large Models\n",
    "\n",
    "For large models like Llama-70B, use NDIF remote execution. This performs all computation on the server, sending only the essential top-k results back to you.\n",
    "\n",
    "```python\n",
    "# Set up NDIF (requires API key in .env.local)\n",
    "from nnsight import CONFIG\n",
    "import os\n",
    "CONFIG.set_default_api_key(os.environ['NDIF_API'])\n",
    "\n",
    "# Load a large model\n",
    "model = LanguageModel('meta-llama/Llama-3.1-70B', device_map='auto', token=os.environ['HF_TOKEN'])\n",
    "\n",
    "# Collect with remote=True\n",
    "data = collect_logit_lens_topk_efficient(\n",
    "    prompt, model, top_k=5, track_across_layers=True,\n",
    "    remote=True  # Server-side computation\n",
    ")\n",
    "```\n",
    "\n",
    "Bandwidth comparison:\n",
    "- Naive (full logits): ~819 MB\n",
    "- With server-side reduction: ~320 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Specific Layers\n",
    "\n",
    "You can analyze a subset of layers for faster exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze every other layer\n",
    "data = collect_logit_lens_topk_efficient(\n",
    "    'Hello world',\n",
    "    model,\n",
    "    layers=[0, 2, 4, 6, 8, 10, 11],  # GPT-2 has 12 layers (0-11)\n",
    "    track_across_layers=True,\n",
    "    remote=False,\n",
    ")\n",
    "\n",
    "show_logit_lens(data, model.tokenizer, title='Layer subset analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore different model architectures (see [MODEL_SUPPORT.md](../docs/MODEL_SUPPORT.md))\n",
    "- Try the [live demo](https://davidbau.github.io/logitlenskit/) with Llama 70B data\n",
    "- Check the [API reference](../docs/PYTHON_API.md) for advanced usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
