{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogitLensKit E2E Test: Basic Workflow\n",
    "\n",
    "This notebook tests the basic end-to-end workflow:\n",
    "1. Load a model\n",
    "2. Collect logit lens data\n",
    "3. Display the interactive widget\n",
    "\n",
    "Run with: `pytest --nbmake notebooks/test_e2e_basic.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load environment variables\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add python/src to path for local development\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root / 'python' / 'src'))\n",
    "\n",
    "# Load .env.local\n",
    "env_path = project_root / '.env.local'\n",
    "if env_path.exists():\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            if '=' in line and not line.startswith('#'):\n",
    "                key, val = line.strip().split('=', 1)\n",
    "                os.environ[key] = val.strip('\"').strip(\"'\")\n",
    "    print('Loaded .env.local')\n",
    "else:\n",
    "    print('Warning: .env.local not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Import logitlenskit\n",
    "from logitlenskit import (\n",
    "    collect_logit_lens_topk_efficient,\n",
    "    show_logit_lens,\n",
    "    detect_model_type,\n",
    "    MODEL_CONFIGS,\n",
    ")\n",
    "\n",
    "print('Imports successful')\n",
    "print(f'Supported model types: {list(MODEL_CONFIGS.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Load GPT-2 model (local, no NDIF required)\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('openai-community/gpt2', device_map='auto')\n",
    "print(f'Loaded model: {model.config.model_type}')\n",
    "\n",
    "# Verify detection\n",
    "detected = detect_model_type(model)\n",
    "assert detected == 'gpt2', f'Expected gpt2, got {detected}'\n",
    "print(f'Model type detected: {detected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Collect logit lens data (local execution)\n",
    "prompt = 'The quick brown fox'\n",
    "\n",
    "data = collect_logit_lens_topk_efficient(\n",
    "    prompt,\n",
    "    model,\n",
    "    top_k=5,\n",
    "    track_across_layers=True,\n",
    "    remote=False,  # Local execution\n",
    ")\n",
    "\n",
    "print(f'Tokens: {data[\"tokens\"]}')\n",
    "print(f'Layers analyzed: {len(data[\"layers\"])}')\n",
    "print(f'Top indices shape: {data[\"top_indices\"].shape}')\n",
    "print(f'Tracked tokens per position: {[len(t) for t in data[\"tracked_indices\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Validate data structure\n",
    "assert 'tokens' in data, 'Missing tokens'\n",
    "assert 'layers' in data, 'Missing layers'\n",
    "assert 'top_indices' in data, 'Missing top_indices'\n",
    "assert 'top_probs' in data, 'Missing top_probs'\n",
    "assert 'tracked_indices' in data, 'Missing tracked_indices'\n",
    "assert 'tracked_probs' in data, 'Missing tracked_probs'\n",
    "\n",
    "# Shape checks\n",
    "n_layers = len(data['layers'])\n",
    "n_tokens = len(data['tokens'])\n",
    "k = 5\n",
    "\n",
    "assert data['top_indices'].shape == (n_layers, n_tokens, k), \\\n",
    "    f'top_indices shape mismatch: {data[\"top_indices\"].shape}'\n",
    "assert data['top_probs'].shape == (n_layers, n_tokens, k), \\\n",
    "    f'top_probs shape mismatch: {data[\"top_probs\"].shape}'\n",
    "assert len(data['tracked_indices']) == n_tokens, \\\n",
    "    f'tracked_indices length mismatch: {len(data[\"tracked_indices\"])}'\n",
    "assert len(data['tracked_probs']) == n_tokens, \\\n",
    "    f'tracked_probs length mismatch: {len(data[\"tracked_probs\"])}'\n",
    "\n",
    "print('Data structure validation passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Format data for widget\n",
    "from logitlenskit import format_data_for_widget\n",
    "\n",
    "widget_data = format_data_for_widget(data, model.tokenizer)\n",
    "\n",
    "assert 'layers' in widget_data\n",
    "assert 'tokens' in widget_data\n",
    "assert 'cells' in widget_data\n",
    "assert len(widget_data['cells']) == n_tokens\n",
    "assert len(widget_data['cells'][0]) == n_layers\n",
    "\n",
    "# Check cell structure\n",
    "cell = widget_data['cells'][0][0]\n",
    "assert 'token' in cell\n",
    "assert 'prob' in cell\n",
    "assert 'trajectory' in cell\n",
    "assert 'topk' in cell\n",
    "\n",
    "print(f'Widget data: {len(widget_data[\"tokens\"])} tokens, {len(widget_data[\"layers\"])} layers')\n",
    "print(f'Sample cell: {cell[\"token\"]} (prob={cell[\"prob\"]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Display widget (visual verification)\n",
    "# This generates HTML that would render in a real Jupyter environment\n",
    "html = show_logit_lens(data, model.tokenizer, title='GPT-2: The quick brown fox')\n",
    "\n",
    "# Verify HTML was generated\n",
    "assert html is not None\n",
    "html_str = html.data\n",
    "assert 'LogitLensWidget' in html_str, 'Widget code not in HTML'\n",
    "assert 'GPT-2: The quick brown fox' in html_str, 'Title not in HTML'\n",
    "\n",
    "print(f'Generated HTML: {len(html_str)} characters')\n",
    "print('Widget HTML generation successful!')\n",
    "\n",
    "# Display (in real Jupyter this would show the interactive widget)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Layer subset\n",
    "layers_subset = [0, 3, 6, 9, 11]  # GPT-2 has 12 layers\n",
    "\n",
    "data_subset = collect_logit_lens_topk_efficient(\n",
    "    'Test',\n",
    "    model,\n",
    "    top_k=3,\n",
    "    layers=layers_subset,\n",
    "    remote=False,\n",
    ")\n",
    "\n",
    "assert data_subset['layers'] == layers_subset\n",
    "assert data_subset['top_indices'].shape[0] == len(layers_subset)\n",
    "\n",
    "print(f'Layer subset test passed: {layers_subset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('=' * 50)\n",
    "print('E2E Basic Test Summary')\n",
    "print('=' * 50)\n",
    "print('1. Imports: PASSED')\n",
    "print('2. Model loading: PASSED')\n",
    "print('3. Data collection: PASSED')\n",
    "print('4. Data validation: PASSED')\n",
    "print('5. Widget formatting: PASSED')\n",
    "print('6. HTML generation: PASSED')\n",
    "print('7. Layer subset: PASSED')\n",
    "print('=' * 50)\n",
    "print('All tests passed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
