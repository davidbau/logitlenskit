{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens Tutorial\n",
    "\n",
    "This tutorial shows how to visualize the **logit lens** of transformer language models.\n",
    "\n",
    "## What is the Logit Lens?\n",
    "\n",
    "The logit lens is an interpretability technique that decodes hidden states at each layer into vocabulary probabilities. By applying the model's output projection (`ln_final` → `lm_head`) to intermediate layers, we can see how the model's predictions evolve through its computation.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnterp \"logitlenskit @ git+https://github.com/davidbau/logitlenskit.git#subdirectory=python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Using the Library\n",
    "\n",
    "The simplest way to use logitlenskit is with just a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from logitlenskit import collect_logit_lens, show_logit_lens\n",
    "\n",
    "# Load a model (GPT-2 is small enough to run locally)\n",
    "model = StandardizedTransformer(\"openai-community/gpt2\")\n",
    "\n",
    "# Collect logit lens data\n",
    "data = collect_logit_lens(\"The capital of France is\", model, remote=False)\n",
    "\n",
    "# Display interactive visualization\n",
    "show_logit_lens(data, title=\"GPT-2: The capital of France is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the Widget\n",
    "\n",
    "- **Click cells** to see top-k predictions at that layer/position\n",
    "- **Click tokens** in the popup to pin their probability trajectories\n",
    "- **Click input tokens** (left column) to compare multiple positions\n",
    "- **Drag edges** to resize columns and chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Data Format\n",
    "\n",
    "Let's examine what `collect_logit_lens` returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys:\", list(data.keys()))\n",
    "print()\n",
    "print(\"model:\", data[\"model\"])\n",
    "print(\"input:\", data[\"input\"])\n",
    "print(\"layers:\", data[\"layers\"][:5], \"...\", f\"({len(data['layers'])} total)\")\n",
    "print()\n",
    "print(\"topk shape:\", data[\"topk\"].shape, \"- [n_layers, n_positions, k]\")\n",
    "print(\"tracked[0] shape:\", data[\"tracked\"][0].shape, \"- unique tokens at position 0\")\n",
    "print(\"probs[0] shape:\", data[\"probs\"][0].shape, \"- [n_layers, n_tracked] trajectories\")\n",
    "print()\n",
    "print(\"vocab (sample):\", dict(list(data[\"vocab\"].items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"model\": str,                # Model name\n",
    "    \"input\": List[str],          # Input tokens as strings\n",
    "    \"layers\": List[int],         # Layer indices analyzed\n",
    "    \"topk\": Tensor[int32],       # [n_layers, n_pos, k] - top-k token indices\n",
    "    \"tracked\": List[Tensor],     # Per-position unique token indices\n",
    "    \"probs\": List[Tensor],       # Per-position [n_layers, n_tracked] probabilities\n",
    "    \"vocab\": Dict[int, str],     # Token index → string mapping\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: How It Works (Implementation)\n",
    "\n",
    "Here's the complete implementation of `collect_logit_lens`. This is exactly what the library does internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collect_logit_lens(prompt, model, k=5, layers=None, remote=True):\n",
    "    \"\"\"\n",
    "    Collect logit lens data: top-k predictions and probability trajectories.\n",
    "    \"\"\"\n",
    "    # Tokenize once, client-side\n",
    "    token_ids = model.tokenizer.encode(prompt)\n",
    "    n_pos = len(token_ids)\n",
    "\n",
    "    # Default: all layers\n",
    "    if layers is None:\n",
    "        layers = list(range(model.num_layers))\n",
    "    n_layers = len(layers)\n",
    "\n",
    "    # Run model, compute logit lens\n",
    "    # When remote=True, computation happens on NDIF server\n",
    "    with model.trace(token_ids, remote=remote):\n",
    "        all_probs = []\n",
    "        all_topk = []\n",
    "\n",
    "        for li in layers:\n",
    "            # Project hidden state to vocabulary: hidden -> norm -> lm_head\n",
    "            logits = model.lm_head(model.ln_final(model.layers_output[li]))\n",
    "            probs = torch.softmax(logits[0], dim=-1)\n",
    "            all_probs.append(probs)\n",
    "            all_topk.append(probs.topk(k, dim=-1).indices)\n",
    "\n",
    "        # Stack top-k indices: [n_layers, n_pos, k]\n",
    "        topk = torch.stack(all_topk).to(torch.int32)\n",
    "\n",
    "        # For each position: find unique tokens, extract trajectories\n",
    "        tracked = []\n",
    "        probs_out = []\n",
    "        for pos in range(n_pos):\n",
    "            # Union of all tokens appearing in top-k at any layer\n",
    "            unique = torch.unique(topk[:, pos, :].flatten()).to(torch.int32)\n",
    "            # Extract probability trajectory for each unique token\n",
    "            traj = torch.stack([all_probs[li][pos, unique] for li in range(n_layers)])\n",
    "            tracked.append(unique)\n",
    "            probs_out.append(traj)\n",
    "\n",
    "        # Save results to transmit from server\n",
    "        result = {\"topk\": topk, \"tracked\": tracked, \"probs\": probs_out}.save()\n",
    "\n",
    "    # Build vocabulary map (client-side)\n",
    "    all_ids = set(result[\"topk\"].flatten().tolist())\n",
    "    for t in result[\"tracked\"]:\n",
    "        all_ids.update(t.tolist())\n",
    "    vocab = {i: model.tokenizer.decode([i]) for i in all_ids}\n",
    "\n",
    "    return {\n",
    "        \"model\": model.config._name_or_path,\n",
    "        \"input\": [model.tokenizer.decode([t]) for t in token_ids],\n",
    "        \"layers\": layers,\n",
    "        \"topk\": result[\"topk\"],\n",
    "        \"tracked\": result[\"tracked\"],\n",
    "        \"probs\": result[\"probs\"],\n",
    "        \"vocab\": vocab,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "1. **Standardized access via nnterp**: `model.layers_output[i]`, `model.ln_final`, `model.lm_head` work for any transformer architecture\n",
    "\n",
    "2. **Server-side computation**: When `remote=True`, all the heavy work (forward pass, softmax, top-k) happens on NDIF servers. Only the small results are transmitted back.\n",
    "\n",
    "3. **Trajectory tracking**: We find all tokens that appear in top-k at *any* layer, then extract their probabilities at *all* layers. This enables visualizing how predictions evolve.\n",
    "\n",
    "4. **Bandwidth optimization**: Instead of sending full logits (~500MB for a 70B model), we send only top-k indices and tracked probabilities (~500KB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Converting to JavaScript Format\n",
    "\n",
    "The widget uses a JSON format optimized for the browser. Here's the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_js_format(data):\n",
    "    \"\"\"\n",
    "    Convert Python API format to JavaScript V2 format.\n",
    "    \"\"\"\n",
    "    vocab = data[\"vocab\"]\n",
    "    n_layers = len(data[\"layers\"])\n",
    "    n_pos = len(data[\"input\"])\n",
    "\n",
    "    # topk: [n_layers, n_pos, k] indices -> [n_layers][n_pos] string lists\n",
    "    topk_js = [\n",
    "        [[vocab[idx.item()] for idx in data[\"topk\"][li, pos]]\n",
    "         for pos in range(n_pos)]\n",
    "        for li in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    # tracked/probs: parallel arrays -> {token: trajectory} dicts per position\n",
    "    tracked_js = [\n",
    "        {\n",
    "            vocab[idx.item()]: [round(p, 5) for p in data[\"probs\"][pos][:, i].tolist()]\n",
    "            for i, idx in enumerate(data[\"tracked\"][pos])\n",
    "        }\n",
    "        for pos in range(n_pos)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"meta\": {\"version\": 2, \"model\": data[\"model\"]},\n",
    "        \"input\": data[\"input\"],\n",
    "        \"layers\": data[\"layers\"],\n",
    "        \"topk\": topk_js,\n",
    "        \"tracked\": tracked_js,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and examine\n",
    "js_data = to_js_format(data)\n",
    "\n",
    "print(\"JavaScript format keys:\", list(js_data.keys()))\n",
    "print()\n",
    "print(\"meta:\", js_data[\"meta\"])\n",
    "print(\"input:\", js_data[\"input\"])\n",
    "print()\n",
    "print(\"topk[0][0] (layer 0, position 0):\", js_data[\"topk\"][0][0])\n",
    "print()\n",
    "print(\"tracked[0] (position 0, first 3 tokens):\")\n",
    "for tok, traj in list(js_data[\"tracked\"][0].items())[:3]:\n",
    "    print(f\"  {tok!r}: {traj[:5]}... ({len(traj)} values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JavaScript Format Structure\n",
    "\n",
    "```javascript\n",
    "{\n",
    "    \"meta\": {\"version\": 2, \"model\": \"...\"},\n",
    "    \"input\": [\"The\", \" capital\", ...],\n",
    "    \"layers\": [0, 1, 2, ...],\n",
    "    \"topk\": [                           // [n_layers][n_positions] → token strings\n",
    "        [[\"the\", \"a\", ...], ...],       // layer 0\n",
    "        ...\n",
    "    ],\n",
    "    \"tracked\": [                        // [n_positions] → {token: trajectory}\n",
    "        {\" Paris\": [0.01, 0.02, ...], ...},  // position 0\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Key differences from Python format:\n",
    "- Token indices replaced with strings (no vocab dict needed)\n",
    "- Trajectories stored as `{token: [prob_at_layer_0, prob_at_layer_1, ...]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using NDIF for Large Models\n",
    "\n",
    "For large models like Llama-70B, use NDIF remote execution:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from nnsight import CONFIG\n",
    "\n",
    "# Set up NDIF API key\n",
    "CONFIG.set_default_api_key(os.environ['NDIF_API_KEY'])\n",
    "\n",
    "# Load large model\n",
    "model = StandardizedTransformer(\"meta-llama/Llama-3.1-70B\")\n",
    "\n",
    "# Collect with remote=True (default)\n",
    "data = collect_logit_lens(\"The capital of France is\", model, remote=True)\n",
    "show_logit_lens(data)\n",
    "```\n",
    "\n",
    "### Bandwidth Savings\n",
    "\n",
    "For Llama-70B (80 layers, 128k vocab):\n",
    "- Full logits: ~547 MB\n",
    "- Our format: ~500 KB\n",
    "- **1000× reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Try Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The quick brown fox jumps over the\",\n",
    "    \"To be or not to be, that is the\",\n",
    "    \"1 + 1 =\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    data = collect_logit_lens(prompt, model, remote=False)\n",
    "    display(show_logit_lens(data, title=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **What the logit lens is**: Projecting intermediate hidden states to vocabulary probabilities\n",
    "\n",
    "2. **How to use the library**: `collect_logit_lens()` + `show_logit_lens()`\n",
    "\n",
    "3. **How it works internally**: The ~50 lines of code that do the actual computation\n",
    "\n",
    "4. **The data formats**: Python format (tensors + vocab) and JavaScript format (strings)\n",
    "\n",
    "5. **Why it's efficient**: Server-side top-k extraction reduces bandwidth by 1000×"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
